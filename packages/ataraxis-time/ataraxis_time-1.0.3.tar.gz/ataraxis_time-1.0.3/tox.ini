# This file provides configurations for tox-based project automation. Generally, this project uses tox similar to how
# some other projects use build-systems.

# Base tox configurations. Note, the 'envlist' will be run in the listed order whenever 'tox' is used without an -e
# specifier.
[tox]
requires =
    tox-uv
    tox>=4
envlist =
    stubs
    lint
    {py310, py311, py312}-test
    combine-test-reports
    doxygen
    docs

# This forces tox to create a 'sterile' environment into which the project with all dependencies is installed prior to
# running the requested tasks, isolating the process from the rest of the system. This is almost always the desired
# runtime mode.
isolated_build = True

# Note: Uses mypy v 1.4.0 due to a persistent failure of 1.5.0+ to work as intended (runs into a fatal error, cause
# not known). v 1.10 works fine for source documentation, but since we are using c-extensions and mypy does not
# process c-sources, the lesser of two evils was chosen and v 1.4.0 is used for now.
[testenv: stubs]
skip_install = true
description =
    Ensures that the source code contains the py.typed marker at the top of the '/src' hierarchy. Then, installs
    the package, compiling the necessary c-extensions and generates stubs for all files inside the (distribution)
    package directory. Finally, moves the files to the same location, relative to the '/src' directory, as they are
    found inside the distributed pacakge. Jointly, these steps make static type-checkers like mypy properly recognize
    and work with the installed pacakge.
deps =
    mypy<1.5.0
    pyyaml
    click
    uv
commands =
    python automation.py process-typed-markers
    uv pip install .
    stubgen -o stubs --include-private -p ataraxis_time -v
    python automation.py process-stubs

# Note: The 'basepython' argument should either be set to the oldest version in the supported stack or to the main
# version. It controls the specific ruleset used to format and (especially) style-check the code.
[testenv: lint]
skip_install = true
description =
    Runs static code formatting, style and typing checkers. Type checker may not work properly until stubs are
    generated via the 'stubs' task. Nevertheless, the tasks are independent and can be run in any order.
deps =
    mypy
    ruff
    numpy
    types-tqdm
depends = stubs
basepython = py310
commands =
    ruff check --select I --fix
    ruff format
    mypy . --strict --extra-checks

# Note: The test source code should be written to import and use intended library name, as the project is compiled and
# installed as a library prior to running the tests. Therefore, the tests need to be designed to test the distributed
# library, rather than the source code.
[testenv: {py310, py311, py312}-test]
package=wheel
description =
    Runs unit and integration tests for each of the python versions listed in the task name. To optimize testing speed,
    defaults to using all logical cores of the host-platform and generates coverage reports. Uses 'loadgroup' balancing
    method to distribute the tests across all workers, which allows manually assigning some tests to use the same
    worker (see pytest-xdist documentation for details).
deps =
    pytest
    pytest-cov
    pytest-xdist
    coverage[toml]
    uv
setenv =
# Sets environment parameters, which includes intermediate coverage aggregation file used by combine-test-reports.
    COVERAGE_FILE = reports{/}.coverage.{envname}
commands =
    # Make sure the --cov is always set to the intended library name, so that coverage runs on the whole library
    # exactly once.
    uv pip install .
    pytest --import-mode=append --cov=ataraxis_time --cov-config=pyproject.toml --cov-report=xml \
    --junitxml=reports/pytest.xml.{envname} -n logical --dist loadgroup

[testenv:combine-test-reports]
description =
  Combines test-coverage data from multiple test runs (for different python versions) into a single html file. The file
  can be viewed by loading the 'reports/coverage_html/index.html'.
skip_install = true
setenv = COVERAGE_FILE = reports/.coverage
depends = {py310, py311, py312}-test
deps =
    junitparser
    coverage[toml]
commands =
    junitparser merge --glob reports/pytest.xml.* reports/pytest.xml
    coverage combine --keep
    coverage xml
    coverage html

# Note: since doxygen is not pip-installable, it has to be installed and made available system-wide for this task to
# succeed. Consult https://www.doxygen.nl/manual/install.html for guidance.
[testenv:doxygen]
skip_install = true
description =
    Generates C++ / C source code documentation using Doxygen. This assumes the source code uses doxygen-compatible
    docstrings and that the root directory contains a Doxyfile that minimally configures Doxygen runtime. This task
    is only needed for projects with C-extensions and works on top of the main 'docs' task.
allowlist_externals = doxygen
commands =
# Instructs doxygen to use the local Doxyfile instance to parse C++ docstrings
    doxygen Doxyfile

[testenv:docs]
description =
    Builds the API documentation from source code docstrings using Sphinx. Integrates with C / C++ documentation via
    Breathe, provided that Doxygen was used to generate the initial .xml file for C-extension sources. The result can
    be viewed by loading 'docs/build/html/index.html'.
depends = doxygen
deps =
    sphinx
    importlib_metadata
    breathe
    sphinx-rtd-theme
    sphinx-click
commands =
# Instructs the sphinx to build the html documentation using local configuration files. Uses '-j auto' to parallelize
# the build process and '-v' to make it verbose.
    sphinx-build -b html -d docs/build/doctrees docs/source docs/build/html -j auto -v

# Note: you can use the '--platform' directive value to build linux (requires docker) wheels on non-native platforms.
[testenv:build]
skip-install = true
description =
    Builds the source code distribution (sdist) and compiles and assembles binary wheels for all possible architectures
    of the host-platform. The wheels contain compiled c-extension code and pure-python code. Use 'upload' task to
    subsequently upload built wheels to PIP.
deps =
    cibuildwheel[uv]
    build
allowlist_externals =
    docker
commands =
    python -m build . --sdist
    cibuildwheel --output-dir dist --platform auto

# You can pass the '--replace-token' flag from the command line to replace the token stored in the .pypirc file.
[testenv:upload]
skip_install = true
description =
    Uses twine to upload all files inside the '/dist' folder to pip, ignoring any files that are already uploaded.
    The first time this command runs on a new machine, this generates a private '.pypirc' file and asks for the
    API token. Subsequently, the locally-stored token will be used each time this command is called.
deps =
    twine
    grayskull
    click
    pyyaml
allowlist_externals =
    distutils
commands =
    python automation.py set-pypi-token {posargs:}
    twine upload dist/* --skip-existing --config-file .pypirc

# Note: This task automatically uses the latest version of the pacakge uploaded to PIP and expects it to contain
# sdist archive. Ideally, it should be used together with the build and twine tasks, as that would ensure the recipe
# always matches the latest distributed code version.
[testenv:recipe]
skip_install = true
description =
    Uses grayskull to parse the source code tarball stored on pip and generate the recipe used to submit the
    package to conda-forge. The submission process has to be carried out manually, see
    https://conda-forge.org/docs/maintainer/adding_pkgs/ for more details.
deps =
    twine
    grayskull
    pyyaml
    click
commands =
    python automation.py generate-recipe-folder
    grayskull pypi ataraxis-time -o recipe --strict-conda-forge --list-missing-deps -m Inkaros

[testenv:export-env]
skip_install = true
deps =
    click
    pyyaml
description =
    Exports the project's conda environment to the 'envs' folder as a .yml file and as a spec.txt with revision history.
    This process automatically determines the appropriate os-suffix and removes the prefix from the exported .yml file.
    Use this task after making changes to your local conda environment to update stored environment specification.
commands =
    python automation.py export-env --base-env axt_dev

[testenv:import-env]
skip_install = true
deps =
    click
    pyyaml
description =
    Automatically resolves the appropriate environment os-suffix and attempts to import the first discovered .yml
    file with the appropriate os-suffix into local conda distribution. If an environment with the same name as the
    imported environment already exists, it will be updated with the data extracted from the .yml file. Use this task to
    update the local environment or create it from-scratch when changes are shared through the exported files in the
    'envs' folder.
commands =
    python automation.py import-env

[testenv:rename-envs]
skip_install = true
deps =
    click
    pyyaml
description =
    This is a utility task that replaces the base environment name used by all files inside the 'envs' directory
    with the user-input name.
commands =
    python automation.py rename-environments

[testenv:adopt]
skip_install = true
deps =
    click
    pyyaml
description =
    This is a unique utility task that is used by Sun Lab members when adopting a project generated from one of the
    private templates. Since templates are not (at this time) exposed to the public, this functionality is not intended
    to end-users.
commands =
    python automation.py adopt-project
