{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "(first)=\n",
    "\n",
    "# Getting started\n",
    "\n",
    "This tutorial is based on [the quickstart example in the celerite documentation](https://celerite.readthedocs.io/en/stable/tutorials/first/), but it has been updated to work with *celerite2*.\n",
    "\n",
    "For this tutorial, we’re going to fit a Gaussian Process (GP) model to a simulated dataset with quasiperiodic oscillations.\n",
    "We’re also going to leave a gap in the simulated data and we’ll use the GP model to predict what we would have observed for those \"missing\" datapoints.\n",
    "\n",
    "To start, here’s some code to simulate the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "t = np.sort(\n",
    "    np.append(\n",
    "        np.random.uniform(0, 3.8, 57),\n",
    "        np.random.uniform(5.5, 10, 68),\n",
    "    )\n",
    ")  # The input coordinates must be sorted\n",
    "yerr = np.random.uniform(0.08, 0.22, len(t))\n",
    "y = (\n",
    "    0.2 * (t - 5)\n",
    "    + np.sin(3 * t + 0.1 * (t - 5) ** 2)\n",
    "    + yerr * np.random.randn(len(t))\n",
    ")\n",
    "\n",
    "true_t = np.linspace(0, 10, 500)\n",
    "true_y = 0.2 * (true_t - 5) + np.sin(3 * true_t + 0.1 * (true_t - 5) ** 2)\n",
    "\n",
    "plt.plot(true_t, true_y, \"k\", lw=1.5, alpha=0.3)\n",
    "plt.errorbar(t, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.xlabel(\"x [day]\")\n",
    "plt.ylabel(\"y [ppm]\")\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "_ = plt.title(\"simulated data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Now, let's fit this dataset using a mixture of `SHOTerm` terms: one quasi-periodic component and one non-periodic component.\n",
    "First let's set up an initial model to see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import celerite2\n",
    "from celerite2 import terms\n",
    "\n",
    "# Quasi-periodic term\n",
    "term1 = terms.SHOTerm(sigma=1.0, rho=1.0, tau=10.0)\n",
    "\n",
    "# Non-periodic component\n",
    "term2 = terms.SHOTerm(sigma=1.0, rho=5.0, Q=0.25)\n",
    "kernel = term1 + term2\n",
    "\n",
    "# Setup the GP\n",
    "gp = celerite2.GaussianProcess(kernel, mean=0.0)\n",
    "gp.compute(t, yerr=yerr)\n",
    "\n",
    "print(\"Initial log likelihood: {0}\".format(gp.log_likelihood(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Let's look at the underlying power spectral density of this initial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = np.linspace(1.0 / 8, 1.0 / 0.3, 500)\n",
    "omega = 2 * np.pi * freq\n",
    "\n",
    "\n",
    "def plot_psd(gp):\n",
    "    for n, term in enumerate(gp.kernel.terms):\n",
    "        plt.loglog(freq, term.get_psd(omega), label=\"term {0}\".format(n + 1))\n",
    "    plt.loglog(freq, gp.kernel.get_psd(omega), \":k\", label=\"full model\")\n",
    "    plt.xlim(freq.min(), freq.max())\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"frequency [1 / day]\")\n",
    "    plt.ylabel(\"power [day ppt$^2$]\")\n",
    "\n",
    "\n",
    "plt.title(\"initial psd\")\n",
    "plot_psd(gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "And then we can also plot the prediction that this model makes for the missing data and compare it to the truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(gp):\n",
    "    plt.plot(true_t, true_y, \"k\", lw=1.5, alpha=0.3, label=\"data\")\n",
    "    plt.errorbar(t, y, yerr=yerr, fmt=\".k\", capsize=0, label=\"truth\")\n",
    "\n",
    "    if gp:\n",
    "        mu, variance = gp.predict(y, t=true_t, return_var=True)\n",
    "        sigma = np.sqrt(variance)\n",
    "        plt.plot(true_t, mu, label=\"prediction\")\n",
    "        plt.fill_between(true_t, mu - sigma, mu + sigma, color=\"C0\", alpha=0.2)\n",
    "\n",
    "    plt.xlabel(\"x [day]\")\n",
    "    plt.ylabel(\"y [ppm]\")\n",
    "    plt.xlim(0, 10)\n",
    "    plt.ylim(-2.5, 2.5)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "plt.title(\"initial prediction\")\n",
    "plot_prediction(gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Ok, that looks pretty terrible, but we can get a better fit by numerically maximizing the likelihood as described in the following section.\n",
    "\n",
    "## Maximum likelihood\n",
    "\n",
    "In this section, we'll improve our initial GP model by maximizing the likelihood function for the parameters of the kernel, the mean, and a \"jitter\" (a constant variance term added to the diagonal of our covariance matrix).\n",
    "To do this, we'll use the numerical optimization routine from [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def set_params(params, gp):\n",
    "    gp.mean = params[0]\n",
    "    theta = np.exp(params[1:])\n",
    "    gp.kernel = terms.SHOTerm(\n",
    "        sigma=theta[0], rho=theta[1], tau=theta[2]\n",
    "    ) + terms.SHOTerm(sigma=theta[3], rho=theta[4], Q=0.25)\n",
    "    gp.compute(t, diag=yerr**2 + theta[5], quiet=True)\n",
    "    return gp\n",
    "\n",
    "\n",
    "def neg_log_like(params, gp):\n",
    "    gp = set_params(params, gp)\n",
    "    return -gp.log_likelihood(y)\n",
    "\n",
    "\n",
    "initial_params = [0.0, 0.0, 0.0, np.log(10.0), 0.0, np.log(5.0), np.log(0.01)]\n",
    "soln = minimize(neg_log_like, initial_params, method=\"L-BFGS-B\", args=(gp,))\n",
    "opt_gp = set_params(soln.x, gp)\n",
    "soln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Now let's make the same plots for the maximum likelihood model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"maximum likelihood psd\")\n",
    "plot_psd(opt_gp)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"maximum likelihood prediction\")\n",
    "plot_prediction(opt_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "These predictions are starting to look much better!\n",
    "\n",
    "## Posterior inference using emcee\n",
    "\n",
    "Now, to get a sense for the uncertainties on our model, let's use Markov chain Monte Carlo (MCMC) to numerically estimate the posterior expectations of the model.\n",
    "In this first example, we'll use the [emcee](https://emcee.readthedocs.io) package to run our MCMC.\n",
    "Our likelihood function is the same as the one we used in the previous section, but we'll also choose a wide normal prior on each of our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "prior_sigma = 2.0\n",
    "\n",
    "\n",
    "def log_prob(params, gp):\n",
    "    gp = set_params(params, gp)\n",
    "    return (\n",
    "        gp.log_likelihood(y) - 0.5 * np.sum((params / prior_sigma) ** 2),\n",
    "        gp.kernel.get_psd(omega),\n",
    "    )\n",
    "\n",
    "\n",
    "np.random.seed(5693854)\n",
    "coords = soln.x + 1e-5 * np.random.randn(32, len(soln.x))\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    coords.shape[0], coords.shape[1], log_prob, args=(gp,)\n",
    ")\n",
    "state = sampler.run_mcmc(coords, 2000, progress=False)\n",
    "sampler.reset()\n",
    "state = sampler.run_mcmc(state, 5000, progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "After running our MCMC, we can plot the predictions that the model makes for a handful of samples from the chain.\n",
    "This gives a qualitative sense of the uncertainty in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = sampler.get_chain(discard=100, flat=True)\n",
    "\n",
    "for sample in chain[np.random.randint(len(chain), size=50)]:\n",
    "    gp = set_params(sample, gp)\n",
    "    conditional = gp.condition(y, true_t)\n",
    "    plt.plot(true_t, conditional.sample(), color=\"C0\", alpha=0.1)\n",
    "\n",
    "plt.title(\"posterior prediction\")\n",
    "plot_prediction(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Similarly, we can plot the posterior expectation for the power spectral density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "psds = sampler.get_blobs(discard=100, flat=True)\n",
    "\n",
    "q = np.percentile(psds, [16, 50, 84], axis=0)\n",
    "\n",
    "plt.loglog(freq, q[1], color=\"C0\")\n",
    "plt.fill_between(freq, q[0], q[2], color=\"C0\", alpha=0.1)\n",
    "\n",
    "plt.xlim(freq.min(), freq.max())\n",
    "plt.xlabel(\"frequency [1 / day]\")\n",
    "plt.ylabel(\"power [day ppt$^2$]\")\n",
    "_ = plt.title(\"posterior psd using emcee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Posterior inference using PyMC\n",
    "\n",
    "*celerite2* also includes support for probabilistic modeling using PyMC (v5 or v3, using the `celerite2.pymc` or `celerite2.pymc3` submodule respectively), and we can implement the same model from above as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "from celerite2.pymc import GaussianProcess, terms as pm_terms\n",
    "\n",
    "with pm.Model() as model:\n",
    "    mean = pm.Normal(\"mean\", mu=0.0, sigma=prior_sigma)\n",
    "    log_jitter = pm.Normal(\"log_jitter\", mu=0.0, sigma=prior_sigma)\n",
    "\n",
    "    log_sigma1 = pm.Normal(\"log_sigma1\", mu=0.0, sigma=prior_sigma)\n",
    "    log_rho1 = pm.Normal(\"log_rho1\", mu=0.0, sigma=prior_sigma)\n",
    "    log_tau = pm.Normal(\"log_tau\", mu=0.0, sigma=prior_sigma)\n",
    "    term1 = pm_terms.SHOTerm(\n",
    "        sigma=pm.math.exp(log_sigma1),\n",
    "        rho=pm.math.exp(log_rho1),\n",
    "        tau=pm.math.exp(log_tau),\n",
    "    )\n",
    "\n",
    "    log_sigma2 = pm.Normal(\"log_sigma2\", mu=0.0, sigma=prior_sigma)\n",
    "    log_rho2 = pm.Normal(\"log_rho2\", mu=0.0, sigma=prior_sigma)\n",
    "    term2 = pm_terms.SHOTerm(\n",
    "        sigma=pm.math.exp(log_sigma2), rho=pm.math.exp(log_rho2), Q=0.25\n",
    "    )\n",
    "\n",
    "    kernel = term1 + term2\n",
    "    gp = GaussianProcess(kernel, mean=mean)\n",
    "    gp.compute(t, diag=yerr**2 + pm.math.exp(log_jitter), quiet=True)\n",
    "    gp.marginal(\"obs\", observed=y)\n",
    "\n",
    "    pm.Deterministic(\"psd\", kernel.get_psd(omega))\n",
    "\n",
    "    trace = pm.sample(\n",
    "        tune=1000,\n",
    "        draws=1000,\n",
    "        target_accept=0.9,\n",
    "        init=\"adapt_full\",\n",
    "        cores=2,\n",
    "        chains=2,\n",
    "        random_seed=34923,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Like before, we can plot the posterior estimate of the power spectrum to show that the results are qualitatively similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "psds = trace.posterior[\"psd\"].values\n",
    "\n",
    "q = np.percentile(psds, [16, 50, 84], axis=(0, 1))\n",
    "\n",
    "plt.loglog(freq, q[1], color=\"C0\")\n",
    "plt.fill_between(freq, q[0], q[2], color=\"C0\", alpha=0.1)\n",
    "\n",
    "plt.xlim(freq.min(), freq.max())\n",
    "plt.xlabel(\"frequency [1 / day]\")\n",
    "plt.ylabel(\"power [day ppt$^2$]\")\n",
    "_ = plt.title(\"posterior psd using PyMC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Posterior inference using numpyro\n",
    "\n",
    "Since celerite2 also includes support for JAX, you can also use tools like [numpyro](https://github.com/pyro-ppl/numpyro) for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "import celerite2.jax\n",
    "from celerite2.jax import terms as jax_terms\n",
    "\n",
    "\n",
    "def numpyro_model(t, yerr, y=None):\n",
    "    mean = numpyro.sample(\"mean\", dist.Normal(0.0, prior_sigma))\n",
    "    log_jitter = numpyro.sample(\"log_jitter\", dist.Normal(0.0, prior_sigma))\n",
    "\n",
    "    log_sigma1 = numpyro.sample(\"log_sigma1\", dist.Normal(0.0, prior_sigma))\n",
    "    log_rho1 = numpyro.sample(\"log_rho1\", dist.Normal(0.0, prior_sigma))\n",
    "    log_tau = numpyro.sample(\"log_tau\", dist.Normal(0.0, prior_sigma))\n",
    "    term1 = jax_terms.SHOTerm(\n",
    "        sigma=jnp.exp(log_sigma1), rho=jnp.exp(log_rho1), tau=jnp.exp(log_tau)\n",
    "    )\n",
    "\n",
    "    log_sigma2 = numpyro.sample(\"log_sigma2\", dist.Normal(0.0, prior_sigma))\n",
    "    log_rho2 = numpyro.sample(\"log_rho2\", dist.Normal(0.0, prior_sigma))\n",
    "    term2 = jax_terms.SHOTerm(\n",
    "        sigma=jnp.exp(log_sigma2), rho=jnp.exp(log_rho2), Q=0.25\n",
    "    )\n",
    "\n",
    "    kernel = term1 + term2\n",
    "    gp = celerite2.jax.GaussianProcess(kernel, mean=mean)\n",
    "    gp.compute(t, diag=yerr**2 + jnp.exp(log_jitter), check_sorted=False)\n",
    "\n",
    "    numpyro.sample(\"obs\", gp.numpyro_dist(), obs=y)\n",
    "    numpyro.deterministic(\"psd\", kernel.get_psd(omega))\n",
    "\n",
    "\n",
    "nuts_kernel = NUTS(numpyro_model, dense_mass=True)\n",
    "mcmc = MCMC(\n",
    "    nuts_kernel,\n",
    "    num_warmup=1000,\n",
    "    num_samples=1000,\n",
    "    num_chains=2,\n",
    "    progress_bar=False,\n",
    ")\n",
    "rng_key = random.PRNGKey(34923)\n",
    "%time mcmc.run(rng_key, t, yerr, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "This runtime was similar to the PyMC result from above, and (as we'll see below) the convergence is also similar.\n",
    "Any difference in runtime will probably disappear for more computationally expensive models, but this interface is looking pretty great here!\n",
    "\n",
    "As above, we can plot the posterior expectations for the power spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "psds = np.asarray(mcmc.get_samples()[\"psd\"])\n",
    "\n",
    "q = np.percentile(psds, [16, 50, 84], axis=0)\n",
    "\n",
    "plt.loglog(freq, q[1], color=\"C0\")\n",
    "plt.fill_between(freq, q[0], q[2], color=\"C0\", alpha=0.1)\n",
    "\n",
    "plt.xlim(freq.min(), freq.max())\n",
    "plt.xlabel(\"frequency [1 / day]\")\n",
    "plt.ylabel(\"power [day ppt$^2$]\")\n",
    "_ = plt.title(\"posterior psd using numpyro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Finally, let's compare the results of these different inference methods a bit more quantitaively.\n",
    "First, let's look at the posterior constraint on the period of the underdamped harmonic oscillator, the effective period of the oscillatory signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "emcee_data = az.from_emcee(\n",
    "    sampler,\n",
    "    var_names=[\n",
    "        \"mean\",\n",
    "        \"log_sigma1\",\n",
    "        \"log_rho1\",\n",
    "        \"log_tau\",\n",
    "        \"log_sigma2\",\n",
    "        \"log_rho2\",\n",
    "        \"log_jitter\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "pm_data = trace\n",
    "numpyro_data = az.from_numpyro(mcmc)\n",
    "\n",
    "bins = np.linspace(1.5, 2.75, 25)\n",
    "plt.hist(\n",
    "    np.exp(np.asarray((emcee_data.posterior[\"log_rho1\"].T)).flatten()),\n",
    "    bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=\"emcee\",\n",
    ")\n",
    "plt.hist(\n",
    "    np.exp(np.asarray((pm_data.posterior[\"log_rho1\"].T)).flatten()),\n",
    "    bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=\"PyMC\",\n",
    ")\n",
    "plt.hist(\n",
    "    np.exp(np.asarray((numpyro_data.posterior[\"log_rho1\"].T)).flatten()),\n",
    "    bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=\"numpyro\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.yticks([])\n",
    "plt.xlabel(r\"$\\rho_1$\")\n",
    "_ = plt.ylabel(r\"$p(\\rho_1)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "That looks pretty consistent.\n",
    "\n",
    "Next we can look at the [ArviZ](https://arviz-devs.github.io/arviz/) summary for each method to see how the posterior expectations and convergence diagnostics look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(\n",
    "    emcee_data,\n",
    "    var_names=[\n",
    "        \"mean\",\n",
    "        \"log_sigma1\",\n",
    "        \"log_rho1\",\n",
    "        \"log_tau\",\n",
    "        \"log_sigma2\",\n",
    "        \"log_rho2\",\n",
    "        \"log_jitter\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(\n",
    "    pm_data,\n",
    "    var_names=[\n",
    "        \"mean\",\n",
    "        \"log_sigma1\",\n",
    "        \"log_rho1\",\n",
    "        \"log_tau\",\n",
    "        \"log_sigma2\",\n",
    "        \"log_rho2\",\n",
    "        \"log_jitter\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(\n",
    "    numpyro_data,\n",
    "    var_names=[\n",
    "        \"mean\",\n",
    "        \"log_sigma1\",\n",
    "        \"log_rho1\",\n",
    "        \"log_tau\",\n",
    "        \"log_sigma2\",\n",
    "        \"log_rho2\",\n",
    "        \"log_jitter\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Overall these results are consistent, but the $\\hat{R}$ values are a bit high for the emcee run, so I'd probably run that for longer.\n",
    "Either way, for models like these, PyMC and numpyro are generally going to be much better inference tools (in terms of runtime per effective sample) than emcee, so those are the recommended interfaces if the rest of your model can be easily implemented in such a framework."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8decac68cef2c5dc6a7d0912832ae25ea4234fbc9b99318db5a86bd83b7887b1"
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('celerite2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
